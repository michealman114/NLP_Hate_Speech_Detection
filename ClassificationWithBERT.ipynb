{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationWithBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michealman114/Natural-Language-Models-for-Hate-Speech-Classification/blob/main/ClassificationWithBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Vd-S-SiVLi",
        "outputId": "696d7356-2f99-4595-b616-380791a38d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
        "#from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.utils.data as torch_data\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "DUL5HjXGihh3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "seed = 4814\n",
        "\n",
        "if cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(\"running on GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"running on CPU\")\n",
        "\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjLbi1kDijuu",
        "outputId": "42ae5fe4-fd54-4f12-dcb3-56ff86af7585"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on CPU\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8f6aa6f0b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getCommentsTitlesLabels(file_lines):\n",
        "    comment_list = []\n",
        "    title_list = []\n",
        "    labels = []\n",
        "    for line in file_lines:\n",
        "        content = json.loads(line)\n",
        "\n",
        "        comment = content['text']\n",
        "        comment_list.append(comment)\n",
        "\n",
        "        title = content['title']\n",
        "        title_list.append(title)\n",
        "\n",
        "        labels.append(content['label'])\n",
        "    \n",
        "    return comment_list,title_list,labels"
      ],
      "metadata": {
        "id": "K4L8wQreimno"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick one of the following models\n",
        "- BERT for sequence classification\n",
        "- DistilBERT for sequence classificaiton\n",
        "- Customized DistilBERT for sequence classification"
      ],
      "metadata": {
        "id": "n_h0YyZYX-w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "metadata": {
        "id": "kCIOeF_zsFFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)"
      ],
      "metadata": {
        "id": "8rK1LLW2Xg8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "zPCJ5TnWTEYv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to normal stuff"
      ],
      "metadata": {
        "id": "V7xkGQZaYMzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_lines = open(\"./Data/pruned-fox-news-comments.json\", \"r\").readlines() # original 2015 data\n",
        "original_comments, original_titles, original_labels = getCommentsTitlesLabels(original_lines)\n",
        "\n",
        "num_samples = len(original_labels)\n",
        "\n",
        "print(len(original_comments), len(original_titles), len(original_labels))"
      ],
      "metadata": {
        "id": "rXp5cjf1ivLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e910b7-f30c-47bd-ff85-1c0483d12f3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1525 1525 1525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clipped_comments = []\n",
        "clipped_titles = []\n",
        "clipped_labels = []\n",
        "\n",
        "largest_size = 200\n",
        "\n",
        "#clip unnecessarily long comments to improve training speed\n",
        "for comment,title,label in zip(original_comments, original_titles, original_labels):\n",
        "    comment_length = len(tokenizer.encode(comment))\n",
        "    if comment_length > largest_size :\n",
        "        continue\n",
        "    clipped_comments.append(comment)\n",
        "    clipped_titles.append(title)\n",
        "    clipped_labels.append(label)\n",
        "\n",
        "num_samples = len(clipped_labels)\n",
        "print(len(clipped_comments), len(clipped_titles), len(clipped_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyJPBRMsv2H2",
        "outputId": "66d402ee-20b3-4f25-fd99-d0b32db2bad0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1510 1510 1510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "zipped = list(zip(clipped_comments, clipped_titles, clipped_labels))\n",
        "random.shuffle(zipped)\n",
        "shuffled_comments, shuffled_titles, shuffled_labels = zip(*zipped)\n",
        "\n",
        "\n",
        "print(len(shuffled_comments), len(shuffled_titles), len(shuffled_labels))\n",
        "print(shuffled_comments[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXBOdSZctGhI",
        "outputId": "65c66f92-0c30-4d48-a05d-30d876f45dc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1510 1510 1510\n",
            "('God is , as always, whatever you imagine it is.', 'you just now said it was', 'Not a huge fan - state by state issue.', 'So, you object to the 1st Amendment?', \"I wish he'd publish his book already and stop spamming here.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "encoded_dict = tokenizer(\n",
        "                    shuffled_comments,\n",
        "                    add_special_tokens = True,\n",
        "                    max_length = 200,\n",
        "                    padding = True,\n",
        "                    truncation = True,\n",
        "                    return_tensors = 'pt',\n",
        "                )\n",
        "    \n",
        "\n",
        "\n",
        "tokenized_comments = encoded_dict['input_ids']\n",
        "attention_masks = encoded_dict['attention_mask']\n",
        "all_labels = torch.tensor(shuffled_labels)\n",
        "\n",
        "\n",
        "print('Original: ', shuffled_comments[0])\n",
        "print('Token IDs:', tokenized_comments[0][:12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDbYWvgcvsND",
        "outputId": "90452083-78d4-413f-d111-da1f996ed3b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  God is , as always, whatever you imagine it is.\n",
            "Token IDs: tensor([ 101, 2643, 2003, 1010, 2004, 2467, 1010, 3649, 2017, 5674, 2009, 2003])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_comments.shape)\n",
        "print(attention_masks.shape)\n",
        "print(all_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAXooNwrNWMV",
        "outputId": "57524ef6-adef-4e0c-b77e-8e0fd269e055"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1510, 200])\n",
            "torch.Size([1510, 200])\n",
            "torch.Size([1510])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_raw_Dataset(torch.utils.data.Dataset): # renamed to ProcessingDataset to avoid reuse of name\n",
        "    def __init__(self, comments, attention_masks, labels):\n",
        "        \"\"\"\n",
        "        comments/titles: (batch_size, max_length, embed_dim)\n",
        "        labels: (batch_size,)\n",
        "        \"\"\"\n",
        "        #Initialization\n",
        "        self.comments = comments\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "        self.length = labels.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        comment = self.comments[index]\n",
        "        attention_mask = self.attention_masks[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return comment,attention_mask,label"
      ],
      "metadata": {
        "id": "c7O4ZL8Ukeaf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 85-15 train-validation split.\n",
        "max_train = int(0.85 * num_samples)\n",
        "\n",
        "max_train = 400\n",
        "max_val = 500\n",
        "train_dataset = BERT_raw_Dataset(tokenized_comments[:max_train], attention_masks[:max_train], all_labels[:max_train])\n",
        "val_dataset = BERT_raw_Dataset(tokenized_comments[max_train:max_val], attention_masks[max_train:max_val], all_labels[max_train:max_val])\n",
        "#val_dataset = BERT_raw_Dataset(tokenized_comments[max_train:], attention_masks[max_train:], all_labels[max_train:])"
      ],
      "metadata": {
        "id": "mpRKcDg0zdA3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch_data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = torch_data.DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "TDrUK0CBk0_c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ],
      "metadata": {
        "id": "iOJ_R62PAkXn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training customized classifier built from scratch on top of DistilBERT below:"
      ],
      "metadata": {
        "id": "SBy9Y9OWYX6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, bert = None):\n",
        "    assert bert is not None\n",
        "    super().__init__()\n",
        "    self.dense1 = nn.Linear(768, 100)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dense2 = nn.Linear(100,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.model = bert\n",
        "  \n",
        "  def forward(self, batch):\n",
        "    sent_output = self.model(**batch)\n",
        "    CLS_hidden_state = sent_output.last_hidden_state[:,0,:] #(batch_size, embed_dim)\n",
        "\n",
        "    output = self.dense1(CLS_hidden_state)\n",
        "    output = self.relu(output)\n",
        "    output = self.dense2(output)\n",
        "    output = self.sigmoid(torch.squeeze(output))\n",
        "    \n",
        "    return output\n"
      ],
      "metadata": {
        "id": "eN8j_W2qnEfL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "distil_classifier = BertClassifier(bert = distil_bert)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "distil_optimizer = optim.AdamW(distil_classifier.parameters(), lr = 5e-5, eps = 1e-8)\n",
        "distil_scheduler = get_linear_schedule_with_warmup(                \n",
        "                optimizer = distil_optimizer,\n",
        "                num_warmup_steps = 0,\n",
        "                num_training_steps = num_epochs * len(train_loader)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbt8MXpbRWgU",
        "outputId": "d9af0014-ace9-44ef-ac41-25dddd3f31c3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "distil_classifier.to(device)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):    \n",
        "    epoch_training_loss = 0\n",
        "\n",
        "    \n",
        "    distil_classifier.train()\n",
        "    for tokenized_comment, mask, label in train_loader:\n",
        "        tokenized_comment = tokenized_comment.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device).type(torch.float32)\n",
        "\n",
        "\n",
        "        distil_classifier.zero_grad()\n",
        "\n",
        "        batch = {'input_ids': tokenized_comment, 'attention_mask':mask}\n",
        "        preds = distil_classifier(batch)\n",
        "\n",
        "        loss = loss_fn(preds, label)\n",
        "        epoch_training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        distil_optimizer.step()\n",
        "        distil_scheduler.step()\n",
        "        \n",
        "        print(f\"batch of size {label.shape[0]} finished\")\n",
        "\n",
        "    print(f\"epoch training loss = {epoch_training_loss}\")\n",
        "\n",
        "    distil_classifier.eval()\n",
        "\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tokenized_comment, mask, label in val_loader:\n",
        "            tokenized_comment = tokenized_comment.to(device)\n",
        "            mask = mask.to(device)\n",
        "            label = label.to(device).type(torch.float32)\n",
        "\n",
        "            batch = {'input_ids': tokenized_comment, 'attention_mask':mask}\n",
        "            preds = distil_classifier(batch, labels=label, return_dict=True)\n",
        "\n",
        "            \n",
        "            all_val_preds.append(preds.detach().cpu().numpy())\n",
        "            all_val_labels.append(label.detach().cpu().numpy())\n",
        "\n",
        "    \n",
        "\n",
        "    all_val_preds = np.concatenate(all_val_preds)\n",
        "    all_val_labels = np.concatenate(all_val_labels)\n",
        "\n",
        "    print(f\"EPOCH {epoch + 1} finished\")\n",
        "    print('Accuracy:', accuracy_score(label,preds))\n",
        "    print('Precision, Recall, F1:',precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary'))"
      ],
      "metadata": {
        "id": "vKPZGh_KtK6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "9d723cdf-7bb6-4ad9-bac6-d980cc272627"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [01:10<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8763e802f1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepoch_training_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdistil_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdistil_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train BERT/DistilBERT for sequence classification, the infrastructure is a little different"
      ],
      "metadata": {
        "id": "09zeNiqWYR5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Recommended parameters: lr = 5e-5, 3e-5, 2e-5\n",
        "num_epochs = 2,3,4\n",
        "https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "AdamW because it experimentally generalizes better: https://towardsdatascience.com/why-adamw-matters-736223f31b5d\n",
        "\n",
        "standard lr scheduler from here: https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e\n",
        "\"\"\"\n",
        "optimizer = optim.AdamW(model.parameters(), lr = 5e-5, eps = 1e-8)\n",
        "\n",
        "\n",
        "num_epochs = 3\n",
        "scheduler = get_linear_schedule_with_warmup(                \n",
        "                optimizer = optimizer,\n",
        "                num_warmup_steps = 0,\n",
        "                num_training_steps = num_epochs * len(train_loader)\n",
        ")"
      ],
      "metadata": {
        "id": "2FPiYh7Q07Dc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):    \n",
        "    epoch_training_loss = 0\n",
        "\n",
        "    \n",
        "    model.train()\n",
        "    for tokenized_comment, mask, label in train_loader:\n",
        "        tokenized_comment = tokenized_comment.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        result = model(tokenized_comment, attention_mask=mask, labels=label, return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "\n",
        "        epoch_training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        print(f\"batch of size {label.shape[0]} finished\")\n",
        "\n",
        "    print(f\"epoch training loss = {epoch_training_loss}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tokenized_comment, mask, label in val_loader:\n",
        "            tokenized_comment = tokenized_comment.to(device)\n",
        "            mask = mask.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            outputs = model(tokenized_comment, attention_mask=mask, labels=label, return_dict=True)\n",
        "\n",
        "            logits = outputs.logits #output values tensor[16,2] = (batch_size, num_classes) of output values prior to softmaxing\n",
        "            preds = np.argmax(logits, axis=1)\n",
        "\n",
        "            \n",
        "            all_val_preds.append(preds.detach().cpu().numpy())\n",
        "            all_val_labels.append(label.detach().cpu().numpy())\n",
        "\n",
        "    \n",
        "\n",
        "    all_val_preds = np.concatenate(all_val_preds)\n",
        "    all_val_labels = np.concatenate(all_val_labels)\n",
        "\n",
        "    print(f\"EPOCH {epoch + 1} finished\")\n",
        "    print('Accuracy:', accuracy_score(label,preds))\n",
        "    print('Precision, Recall, F1:',precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hxutL2R07It",
        "outputId": "3d24a703-2278-4c2c-f91a-5c95e1e37e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "batch of size 16 finished\n",
            "epoch training loss = 13.97217546403408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\r 33%|███▎      | 1/3 [15:06<30:12, 906.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1 finished\n",
            "Accuracy: 1.0\n",
            "Precision, Recall, F1: (0.0, 0.0, 0.0, None)\n",
            "batch of size 16 finished\n"
          ]
        }
      ]
    }
  ]
}