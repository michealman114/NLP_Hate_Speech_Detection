{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationWithBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SBy9Y9OWYX6e"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michealman114/Natural-Language-Models-for-Hate-Speech-Classification/blob/main/ClassificationWithBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Vd-S-SiVLi",
        "outputId": "1b9b0dac-ecbd-458b-ad15-3686f055b656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertConfig\n",
        "#from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.utils.data as torch_data\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "DUL5HjXGihh3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "seed = 4814\n",
        "\n",
        "if cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(\"running on GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"running on CPU\")\n",
        "\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjLbi1kDijuu",
        "outputId": "7f9dbd25-60f3-47aa-b5bf-2ca6decd3862"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on CPU\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f409b0bc430>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getCommentsTitlesLabels(file_lines):\n",
        "    comment_list = []\n",
        "    title_list = []\n",
        "    labels = []\n",
        "    for line in file_lines:\n",
        "        content = json.loads(line)\n",
        "\n",
        "        comment = content['text']\n",
        "        comment_list.append(comment)\n",
        "\n",
        "        title = content['title']\n",
        "        title_list.append(title)\n",
        "\n",
        "        labels.append(content['label'])\n",
        "    \n",
        "    return comment_list,title_list,labels"
      ],
      "metadata": {
        "id": "K4L8wQreimno"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick one of the following models\n",
        "- BERT for sequence classification\n",
        "- DistilBERT for sequence classification\n",
        "- Customized DistilBERT for sequence classification"
      ],
      "metadata": {
        "id": "aZNWlCNSHaay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we finally tuned the parameters and set up training perfectly we acheived extremely good performance from DistilBERT:\n",
        "DistilBERT for sequence classification results (fine-tuned on 100, tested on 50):\n",
        "- Accuracy: 0.91\n",
        "- Precision, Recall, F1: (0.8656716417910447, 1.0, 0.928, None)\n",
        "However this performance was on a small test dataset of 50 samples (never before seen, but nonetheless a small sample size), so its hard to say in retrospect if it would have performed this well on the whole. \n",
        "\n",
        "From manual testing with custom sentences however this model worked pretty well and intuitively. I didn't save this one unfortunately since I tried reproducing it immediately to no success.\n",
        "\n",
        "\n",
        "DistilBERT for sequence classification results (fine-tuned on 100, validated on 50, tested on remaining 720)\n",
        "- Validation Performance\n",
        "    - Accuracy: 0.76\n",
        "    - Precision, Recall, F1: (0.717948717948718, 0.9655172413793104, 0.8235294117647058, None)\n",
        "- Test Performance\n",
        "    - Accuracy: 0.6366197183098592\n",
        "    - Precision, Recall, F1: (0.5748175182481752, 0.9264705882352942, 0.7094594594594595, None)\n",
        "\n",
        "This model was very overzealous with classifying things as hate speech (low precision), but it also almost never missed hate speech (high recall).\n",
        "\n",
        "DistilBERT for sequence classification results (fine-tuned on 300, validated on 100, tested on remaining 470)\n",
        "- Validation Performance\n",
        "    - Accuracy: 0.71\n",
        "    - Precision, Recall, F1: (0.7254901960784313, 0.7115384615384616, 0.7184466019417477, None)\n",
        "- Test Performance\n",
        "    - Accuracy: 0.65\n",
        "    - Precision, Recall, F1: (0.6115702479338843, 0.6883720930232559, 0.6477024070021883, None)\n",
        "- 3 epochs were run again with model.eval() mode, validation results:\n",
        "    - Accuracy: 0.75\n",
        "    - Precision, Recall, F1: (0.7454545454545455, 0.7884615384615384, 0.766355140186916, None)\n",
        "- 3 more epochs Test:\n",
        "    - Accuracy: 0.717391304347826\n",
        "    - Precision, Recall, F1: (0.683982683982684, 0.7348837209302326, 0.7085201793721975, None)\n",
        "\n",
        "\n",
        "DistilBERT for sequence classification results (fine-tuned on ~700, validated on ~160)\n",
        "- Validation Performance\n",
        "    - Accuracy: 0.7325581395348837\n",
        "    - Precision, Recall, F1: (0.6746987951807228, 0.7466666666666667, 0.708860759493671, None)\n",
        "- After fine tuning for 3 more epochs:\n",
        "    - Accuracy: 0.7616279069767442\n",
        "    - Precision, Recall, F1: (0.7125, 0.76, 0.7354838709677418, None)\n"
      ],
      "metadata": {
        "id": "n_h0YyZYX-w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "metadata": {
        "id": "kCIOeF_zsFFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2)"
      ],
      "metadata": {
        "id": "8rK1LLW2Xg8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c97ba54-b33d-495d-ceac-94c33be3634d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "zPCJ5TnWTEYv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to normal stuff"
      ],
      "metadata": {
        "id": "V7xkGQZaYMzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_lines = open(\"./Data/pruned-fox-news-comments.json\", \"r\").readlines() # original 2015 data\n",
        "original_comments, original_titles, original_labels = getCommentsTitlesLabels(original_lines)\n",
        "\n",
        "num_samples = len(original_labels)\n",
        "\n",
        "print(len(original_comments), len(original_titles), len(original_labels))"
      ],
      "metadata": {
        "id": "rXp5cjf1ivLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374e0793-6c36-485f-f434-de8a8b6f93eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1525 1525 1525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_positives = 0\n",
        "\n",
        "for comment,title,label in zip(original_comments, original_titles, original_labels):\n",
        "    if label == 1:\n",
        "        num_positives += 1\n",
        "print(num_positives, len(original_labels))\n",
        "\n",
        "print(num_positives/len(original_labels)) #this is so sad!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GndBA66wKgUq",
        "outputId": "210fa65f-6bf9-4362-995f-65dd1d028112"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "435 1525\n",
            "0.28524590163934427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_comments = []\n",
        "new_titles = []\n",
        "new_labels = []\n",
        "\n",
        "num_negatives = 0\n",
        "for comment,title,label in zip(original_comments, original_titles, original_labels):\n",
        "    if label == 1:\n",
        "        new_comments.append(comment)\n",
        "        new_titles.append(title)\n",
        "        new_labels.append(label)\n",
        "    if num_negatives < num_positives and label == 0:\n",
        "        new_comments.append(comment)\n",
        "        new_titles.append(title)\n",
        "        new_labels.append(label)\n",
        "        num_negatives += 1\n",
        "\n",
        "print(num_negatives)\n",
        "original_comments, original_titles, original_labels = new_comments, new_titles, new_labels    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbmrvmeILGsS",
        "outputId": "608d5fc2-20f8-409e-eb7f-0cdf43181b58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clipped_comments = []\n",
        "clipped_titles = []\n",
        "clipped_labels = []\n",
        "\n",
        "largest_size = 200\n",
        "\n",
        "#clip unnecessarily long comments to improve training speed\n",
        "for comment,title,label in zip(original_comments, original_titles, original_labels):\n",
        "    comment_length = len(tokenizer.encode(comment))\n",
        "    if comment_length > largest_size :\n",
        "        continue\n",
        "    clipped_comments.append(comment)\n",
        "    clipped_titles.append(title)\n",
        "    clipped_labels.append(label)\n",
        "\n",
        "num_samples = len(clipped_labels)\n",
        "print(len(clipped_comments), len(clipped_titles), len(clipped_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyJPBRMsv2H2",
        "outputId": "1cbfe67c-33f1-4a70-a2a7-80ca113385ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "860 860 860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "zipped = list(zip(clipped_comments, clipped_titles, clipped_labels))\n",
        "random.shuffle(zipped)\n",
        "shuffled_comments, shuffled_titles, shuffled_labels = zip(*zipped)\n",
        "\n",
        "\n",
        "print(len(shuffled_comments), len(shuffled_titles), len(shuffled_labels))\n",
        "print(shuffled_comments[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXBOdSZctGhI",
        "outputId": "1d529446-f3dd-42ff-e8f7-c898efaff37c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "860 860 860\n",
            "(\"How embarrassing for the Navy ....and America. Liberals have gleefully embraced every known perversion and deviance and are angry that sane people refuse to accept their sickness as 'normal'. Liberals - the very worst among us. Liberalism - America's greatest enemy.\", 'Sure you do. And you look like a fo\\ufeffol.', \"Now we have reverse discrimination, BIG TIME because of this PERCEIVED PROBLEM that our DEAR LEADER intended to remedy by EXECUTIVE FIAT! Just another reason why there should not be a federal department of education that doles out OUR OWN TAX MONEY and makes the states JUMP THROUGH HOOPS TO GET IT! Don't you think ENOUGH IS ENOUGH!\", 'And this is the vaunted \"European Society\" America\\'s liberals want us to emulate?', 'Just curious...why do or should we care what she has to say?')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "encoded_dict = tokenizer(\n",
        "                    shuffled_comments,\n",
        "                    add_special_tokens = True,\n",
        "                    max_length = 200,\n",
        "                    padding = True,\n",
        "                    truncation = True,\n",
        "                    return_tensors = 'pt',\n",
        "                )\n",
        "    \n",
        "\n",
        "\n",
        "tokenized_comments = encoded_dict['input_ids']\n",
        "attention_masks = encoded_dict['attention_mask']\n",
        "all_labels = torch.tensor(shuffled_labels)\n",
        "\n",
        "\n",
        "print('Original: ', shuffled_comments[0])\n",
        "print('Token IDs:', tokenized_comments[0][:12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDbYWvgcvsND",
        "outputId": "09f5e1fe-2f3d-41dc-e51f-90b4f5152415"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  How embarrassing for the Navy ....and America. Liberals have gleefully embraced every known perversion and deviance and are angry that sane people refuse to accept their sickness as 'normal'. Liberals - the very worst among us. Liberalism - America's greatest enemy.\n",
            "Token IDs: tensor([  101,  2129, 16436,  2005,  1996,  3212,  1012,  1012,  1012,  1012,\n",
            "         1998,  2637])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_comments.shape)\n",
        "print(attention_masks.shape)\n",
        "print(all_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAXooNwrNWMV",
        "outputId": "2bb97602-8223-418d-db68-ea9f8d5b706c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([860, 200])\n",
            "torch.Size([860, 200])\n",
            "torch.Size([860])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_raw_Dataset(torch.utils.data.Dataset): # renamed to ProcessingDataset to avoid reuse of name\n",
        "    def __init__(self, comments, attention_masks, labels):\n",
        "        \"\"\"\n",
        "        comments/titles: (batch_size, max_length, embed_dim)\n",
        "        labels: (batch_size,)\n",
        "        \"\"\"\n",
        "        #Initialization\n",
        "        self.comments = comments\n",
        "        self.attention_masks = attention_masks\n",
        "        self.labels = labels\n",
        "        self.length = labels.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        comment = self.comments[index]\n",
        "        attention_mask = self.attention_masks[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return comment,attention_mask,label"
      ],
      "metadata": {
        "id": "c7O4ZL8Ukeaf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 85-15 train-validation split.\n",
        "max_train = int(0.80 * num_samples)\n",
        "\n",
        "max_val = all_labels.shape[0]\n",
        "train_dataset = BERT_raw_Dataset(tokenized_comments[:max_train], attention_masks[:max_train], all_labels[:max_train])\n",
        "val_dataset = BERT_raw_Dataset(tokenized_comments[max_train:max_val], attention_masks[max_train:max_val], all_labels[max_train:max_val])\n",
        "#test_dataset = BERT_raw_Dataset(tokenized_comments[max_val:], attention_masks[max_val:], all_labels[max_val:])"
      ],
      "metadata": {
        "id": "mpRKcDg0zdA3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch_data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch_data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "#test_loader = torch_data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "TDrUK0CBk0_c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ],
      "metadata": {
        "id": "iOJ_R62PAkXn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train BERT/DistilBERT for sequence classification, the infrastructure is a little different"
      ],
      "metadata": {
        "id": "09zeNiqWYR5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Recommended parameters: lr = 5e-5, 3e-5, 2e-5\n",
        "num_epochs = 2,3,4\n",
        "https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "AdamW because it experimentally generalizes better: https://towardsdatascience.com/why-adamw-matters-736223f31b5d\n",
        "\n",
        "standard lr scheduler from here: https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e\n",
        "\"\"\"\n",
        "optimizer = optim.AdamW(model.parameters(), lr = 2e-5)\n",
        "\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "2FPiYh7Q07Dc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = get_linear_schedule_with_warmup(                \n",
        "                optimizer = optimizer,\n",
        "                num_warmup_steps = 0,\n",
        "                num_training_steps = num_epochs * len(train_loader)\n",
        ")"
      ],
      "metadata": {
        "id": "GfmFZ0vIFNbZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []\n",
        "loss_fn = nn.BCELoss()\n",
        "model.eval()\n",
        "#model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):    \n",
        "    epoch_training_loss = 0\n",
        "    print()\n",
        "\n",
        "    \n",
        "    #model.train()\n",
        "    for tokenized_comment, mask, label in train_loader:\n",
        "        tokenized_comment = tokenized_comment.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(tokenized_comment, attention_mask=mask, labels=label, return_dict=True)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "\n",
        "        epoch_training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #scheduler.step()\n",
        "        \n",
        "        print(f\"batch of size {label.shape[0]} finished\")\n",
        "\n",
        "    print(f\"epoch training loss = {epoch_training_loss}\")\n",
        "\n",
        "\n",
        "    #model.eval()\n",
        "\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for tokenized_comment, mask, label in val_loader:\n",
        "            tokenized_comment = tokenized_comment.to(device)\n",
        "            mask = mask.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            outputs = model(tokenized_comment, attention_mask=mask, labels=label, return_dict=True)\n",
        "\n",
        "            logits = outputs.logits #output values tensor[16,2] = (batch_size, num_classes) of output values prior to softmaxing\n",
        "            preds = np.argmax(logits, axis=1)\n",
        "\n",
        "            \n",
        "            all_val_preds.append(preds.detach().cpu().numpy())\n",
        "            all_val_labels.append(label.detach().cpu().numpy())\n",
        "\n",
        "    \n",
        "    \n",
        "    all_val_preds = np.concatenate(all_val_preds)\n",
        "    all_val_labels = np.concatenate(all_val_labels)\n",
        "    print(all_val_preds)\n",
        "    print(all_val_labels)\n",
        "\n",
        "    print(f\"EPOCH {epoch + 1} finished\")\n",
        "    print('Accuracy:', accuracy_score(all_val_labels,all_val_preds))\n",
        "    print('Precision, Recall, F1:',precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hxutL2R07It",
        "outputId": "85e1ed9f-06a4-4a6f-d042-821c8faeffc8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 16 finished\n",
            "epoch training loss = 4.590676262974739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [10:38<21:17, 638.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "[0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0]\n",
            "EPOCH 1 finished\n",
            "Accuracy: 0.7558139534883721\n",
            "Precision, Recall, F1: (0.7391304347826086, 0.68, 0.7083333333333334, None)\n",
            "\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 16 finished\n",
            "epoch training loss = 1.772290064021945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [21:21<10:40, 640.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
            "[0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0]\n",
            "EPOCH 2 finished\n",
            "Accuracy: 0.7674418604651163\n",
            "Precision, Recall, F1: (0.7272727272727273, 0.7466666666666667, 0.7368421052631579, None)\n",
            "\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 32 finished\n",
            "batch of size 16 finished\n",
            "epoch training loss = 0.7037207428365946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [31:55<00:00, 638.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0]\n",
            "[0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0]\n",
            "EPOCH 3 finished\n",
            "Accuracy: 0.7616279069767442\n",
            "Precision, Recall, F1: (0.7125, 0.76, 0.7354838709677418, None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Obama and a dog\"\n",
        "encoded_sentence = tokenizer(\n",
        "                    sample_sentence,\n",
        "                    add_special_tokens = True,\n",
        "                    max_length = 200,\n",
        "                    padding = True,\n",
        "                    truncation = True,\n",
        "                    return_tensors = 'pt',\n",
        "                )\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77BZAorqFYNa",
        "outputId": "ab9690b9-cb6f-4c6c-db72-26e0dc107b56"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 8112, 1998, 1037, 3899,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(**encoded_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWtUbJKRSSR2",
        "outputId": "69db5168-7394-4695-8885-1c107fb7c6a2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9432,  0.9099]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_preds = []\n",
        "all_train_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for iter, (tokenized_comment, mask, label) in enumerate(test_loader):\n",
        "        tokenized_comment = tokenized_comment.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        outputs = model(tokenized_comment, attention_mask=mask, labels=label, return_dict=True)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        label = label.detach().cpu().numpy()\n",
        "\n",
        "        if (iter+1) %5 == 0:\n",
        "            print(f\"iteration {iter} results\")\n",
        "            print('loss:', outputs.loss)\n",
        "            print('Accuracy:', accuracy_score(label,preds))\n",
        "            print('Precision, Recall, F1:',precision_recall_fscore_support(label, preds, average='binary'))\n",
        "        \n",
        "        all_train_preds.append(preds)\n",
        "        all_train_labels.append(label)\n",
        "\n",
        "\n",
        "all_train_preds = np.concatenate(all_train_preds)\n",
        "all_train_labels = np.concatenate(all_train_labels)\n",
        "\n",
        "\n",
        "print('\\n===Aggregate Stats===')\n",
        "print('Accuracy:', accuracy_score(all_train_labels,all_train_preds))\n",
        "print('Precision, Recall, F1:',precision_recall_fscore_support(all_train_labels, all_train_preds, average='binary'))"
      ],
      "metadata": {
        "id": "PccTp3N_Zbnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"./BERTs/TrainTrainDistil700.pt\")"
      ],
      "metadata": {
        "id": "pjXBY1g_ZxkE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmAY2ol9jJO3",
        "outputId": "dfdb5ddf-c21c-4a2c-9537-dc7312045e26"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ./BERTs/TrainTrainDistil700.pt /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "6Di3FEHrjcgT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternate Approach\n",
        "Training customized classifier built from scratch on top of DistilBERT below:"
      ],
      "metadata": {
        "id": "SBy9Y9OWYX6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, bert = None):\n",
        "    assert bert is not None\n",
        "    super().__init__()\n",
        "    self.dense1 = nn.Linear(768, 100)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dense2 = nn.Linear(100,1)\n",
        "    self.dense = nn.Linear(768,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.model = bert\n",
        "  \n",
        "  def forward(self, batch):\n",
        "    sent_output = self.model(**batch)\n",
        "    CLS_hidden_state = sent_output.last_hidden_state[:,0,:] #(batch_size, embed_dim)\n",
        "    output = CLS_hidden_state\n",
        "\n",
        "    #output = self.dense1(output)\n",
        "    #output = self.relu(output)\n",
        "    output = self.dense(output)\n",
        "    output = self.sigmoid(torch.squeeze(output))\n",
        "    \n",
        "    return output\n"
      ],
      "metadata": {
        "id": "eN8j_W2qnEfL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distil_bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "distil_bert.eval()\n",
        "distil_classifier = BertClassifier(bert = distil_bert)\n",
        "\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "distil_optimizer = optim.AdamW(distil_classifier.parameters(), lr = 5e-5, eps = 1e-8)\n",
        "distil_scheduler = get_linear_schedule_with_warmup(                \n",
        "                optimizer = distil_optimizer,\n",
        "                num_warmup_steps = 0,\n",
        "                num_training_steps = num_epochs * len(train_loader)\n",
        ")"
      ],
      "metadata": {
        "id": "qbt8MXpbRWgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_stats = []\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "distil_classifier.to(device)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):    \n",
        "    epoch_training_loss = 0\n",
        "    print()\n",
        "\n",
        "    #distil_classifier.train()\n",
        "    for tokenized_comment, mask, label in train_loader:\n",
        "        tokenized_comment = tokenized_comment.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device).type(torch.float32)\n",
        "\n",
        "\n",
        "        distil_classifier.zero_grad()\n",
        "\n",
        "        batch = {'input_ids': tokenized_comment, 'attention_mask':mask}\n",
        "        preds = distil_classifier(batch)\n",
        "\n",
        "        loss = loss_fn(preds, label)\n",
        "        epoch_training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        distil_optimizer.step()\n",
        "        distil_scheduler.step()\n",
        "        \n",
        "        print(f\"batch of size {label.shape[0]} finished\")\n",
        "\n",
        "    print(f\"epoch training loss = {epoch_training_loss}\")\n",
        "    \n",
        "\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    #distil_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        for iter, (tokenized_comment, mask, label) in enumerate(val_loader):\n",
        "            tokenized_comment = tokenized_comment.to(device)\n",
        "            mask = mask.to(device)\n",
        "            label = label.to(device).type(torch.float32)\n",
        "\n",
        "            batch = {'input_ids': tokenized_comment, 'attention_mask':mask}\n",
        "            preds = distil_classifier(batch)\n",
        "            \n",
        "            all_val_preds.append(torch.round(preds).cpu().detach().numpy())\n",
        "            all_val_labels.append(label.detach().cpu().numpy())\n",
        "\n",
        "            break\n",
        "            \n",
        "   \n",
        "\n",
        "    all_val_preds = np.concatenate(all_val_preds)\n",
        "    all_val_labels = np.concatenate(all_val_labels)\n",
        "\n",
        "    print(all_val_preds.shape, all_val_preds)\n",
        "    print(all_val_labels.shape, all_val_labels)\n",
        "\n",
        "    print(f\"EPOCH {epoch + 1} finished\")\n",
        "    print('Accuracy:', accuracy_score(all_val_labels,all_val_preds))\n",
        "    print('Precision, Recall, F1:',precision_recall_fscore_support(all_val_labels, all_val_preds, average='binary'))\n"
      ],
      "metadata": {
        "id": "vKPZGh_KtK6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c18c38d-43aa-4d88-c2d7-9fd265c4b77a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\r 33%|███▎      | 1/3 [00:08<00:16,  8.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1690, 0.1579, 0.1325, 0.1654, 0.1553, 1.2467, 0.1757, 1.1699, 0.1874,\n",
            "        0.1324, 0.1511, 0.2115, 0.1649, 1.1360, 0.1124, 0.1476])\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "(16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(16,) [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "EPOCH 1 finished\n",
            "Accuracy: 0.8125\n",
            "Precision, Recall, F1: (0.0, 0.0, 0.0, None)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "\r 67%|██████▋   | 2/3 [00:12<00:06,  6.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1690, 0.1579, 0.1325, 0.1654, 0.1553, 1.2467, 0.1757, 1.1699, 0.1874,\n",
            "        0.1324, 0.1511, 0.2115, 0.1649, 1.1360, 0.1124, 0.1476])\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "(16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(16,) [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "EPOCH 2 finished\n",
            "Accuracy: 0.8125\n",
            "Precision, Recall, F1: (0.0, 0.0, 0.0, None)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100%|██████████| 3/3 [00:17<00:00,  5.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1690, 0.1579, 0.1325, 0.1654, 0.1553, 1.2467, 0.1757, 1.1699, 0.1874,\n",
            "        0.1324, 0.1511, 0.2115, 0.1649, 1.1360, 0.1124, 0.1476])\n",
            "tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
            "(16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(16,) [0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "EPOCH 3 finished\n",
            "Accuracy: 0.8125\n",
            "Precision, Recall, F1: (0.0, 0.0, 0.0, None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}