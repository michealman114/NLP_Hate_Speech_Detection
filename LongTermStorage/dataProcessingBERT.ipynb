{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e7870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70937c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7883f14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237a6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = open(\"./Data/fox-news-comments.json\", \"r\").readlines() # original 2015 data\n",
    "test_lines = open(\"./Data/modern_comments.json\", \"r\").readlines() # modern data\n",
    "\n",
    "def getCommentsTitlesLabels(file_lines):\n",
    "    comment_list = []\n",
    "    title_list = []\n",
    "    labels = []\n",
    "    for line in file_lines:\n",
    "        content = json.loads(line)\n",
    "\n",
    "        comment = content['text']\n",
    "        comment_list.append(comment)\n",
    "\n",
    "        title = content['title']\n",
    "        title_list.append(title)\n",
    "\n",
    "        labels.append(content['label'])\n",
    "    \n",
    "    return comment_list,title_list,labels\n",
    "\n",
    "train_comments, train_titles, train_labels = getCommentsTitlesLabels(train_lines)\n",
    "test_comments,test_titles,test_labels = getCommentsTitlesLabels(test_lines)\n",
    "\n",
    "tokenized_train_comments = tokenizer(train_comments[:5], padding = True, truncation = True, return_tensors=\"pt\")\n",
    "tokenized_train_titles = tokenizer(train_titles[:5], padding = True, truncation = True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_test_comments = tokenizer(train_comments[:5], padding = True, truncation = True, return_tensors=\"pt\")\n",
    "tokenized_test_titles = tokenizer(train_titles[:5], padding = True, truncation = True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224c5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 106])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_comments['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13920e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in eval mode to turn off dropout regularization etc.\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#use torch.no_grad() to speed up the embedding process\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_train_comments) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
    "\n",
    "    final_comments_embeddings = outputs[0]\n",
    "    comments_embeddings = outputs[1] \n",
    "    # tuple of hidden states at each layer of DistilBERT\n",
    "    # (comments_embeddings[0] corresponds to first layer, comments_embeddings[6] corresponds to last hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef03d9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[torch.Size([5, 106, 768]), torch.Size([5, 106, 768]), torch.Size([5, 106, 768]), torch.Size([5, 106, 768]), torch.Size([5, 106, 768]), torch.Size([5, 106, 768]), torch.Size([5, 106, 768])]\n"
     ]
    }
   ],
   "source": [
    "print(len(comments_embeddings))\n",
    "print([comments_embeddings[i].shape for i in range(len(comments_embeddings))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce3c332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1364, -0.0461,  0.0592,  ..., -0.0681,  0.2070,  0.2929],\n",
      "         [ 0.2435, -0.3248,  0.4040,  ..., -0.3834,  0.6307, -0.0926],\n",
      "         [-0.0554, -0.6182,  0.2613,  ..., -0.1236,  0.3009, -0.4368],\n",
      "         ...,\n",
      "         [ 0.3537,  0.1837,  0.1733,  ..., -0.1353,  0.1789, -0.1086],\n",
      "         [ 0.1004,  0.0388,  0.1707,  ..., -0.0324,  0.0140,  0.0051],\n",
      "         [ 0.1112, -0.0319,  0.1162,  ...,  0.1083,  0.0094,  0.0175]]])\n",
      "tensor([[[-0.1364, -0.0461,  0.0592,  ..., -0.0681,  0.2070,  0.2929],\n",
      "         [ 0.2435, -0.3248,  0.4040,  ..., -0.3834,  0.6307, -0.0926],\n",
      "         [-0.0554, -0.6182,  0.2613,  ..., -0.1236,  0.3009, -0.4368],\n",
      "         ...,\n",
      "         [ 0.3537,  0.1837,  0.1733,  ..., -0.1353,  0.1789, -0.1086],\n",
      "         [ 0.1004,  0.0388,  0.1707,  ..., -0.0324,  0.0140,  0.0051],\n",
      "         [ 0.1112, -0.0319,  0.1162,  ...,  0.1083,  0.0094,  0.0175]]])\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings[:1])\n",
    "print(comments_embeddings[6][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebbc728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = comments_embeddings.last_hidden_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1cfa542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 106, 768])\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleans the dataset and returns the \n",
    "\n",
    "@param file_lines: list of lines in the input file where each line contains all the information for a given comment (content + title + author title + etc.)\n",
    "\n",
    "@returns [labels, comment_list, title_list, max_len, max_title_len]\n",
    "    labels: file\n",
    "    comment_list: list of all comments in the file\n",
    "    title_list: list of all titles in the file\n",
    "    max_comment_len: length of the longest comment in the dataset\n",
    "    max_title_len: length of the longest title in the dataset\n",
    "\"\"\"\n",
    "def clean(file_lines):\n",
    "    max_len = 0\n",
    "    max_title_len = 0  \n",
    "    comment_list = []\n",
    "    title_list = []\n",
    "    label = []\n",
    "    for line in file_lines:\n",
    "        comment = json.loads(line)\n",
    "        \n",
    "        t = comment['text']\n",
    "        t = ' '.join([x for x in t.split() if x[0] != '@'])\n",
    "        t = ' '.join(re.findall(\"[a-zA-Z,.]+\",t))\n",
    "        t = t.replace(',', ' ')\n",
    "        t = t.replace('.', ' ')\n",
    "        text = word_tokenize(t)\n",
    "        text = [x for x in text if x.lower() not in stop_words]\n",
    "        max_len = max(max_len, len(text))\n",
    "        comment_list.append(text)\n",
    "        \n",
    "        title = comment['title']\n",
    "        title = title.replace(',', '')\n",
    "        title = title.replace('.', '')\n",
    "        title = re.findall(\"[a-zA-Z,.]+\",title)\n",
    "        title_list.append(title)\n",
    "        max_title_len = max(max_title_len, len(title))\n",
    "        \n",
    "        label.append(comment['label'])\n",
    "    \n",
    "    labels = np.array(label)\n",
    "    return labels, comment_list, title_list, max_len, max_title_len\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns word2vec embeddings for an input word string\n",
    "\n",
    "@param word : a string\n",
    "@param embed : the embedding keyed vectors (in our case word2vec)\n",
    "@returns : the (300,0) embedding for word\n",
    "\"\"\"\n",
    "def get_embed(word, embed):\n",
    "    x = np.zeros((300,)) # default value should be 0\n",
    "    corrected = spell.correction(word) # closest correction\n",
    "    if word in embed: # base word\n",
    "        x = embed[word]\n",
    "    elif word.upper() in embed: # capitalized (edge case for acronyms like BLM) (for some reason blm doesn't exist but BLM does?)\n",
    "        x = embed[word.upper()]\n",
    "    elif word.lower() in embed: # opposite of capitalization\n",
    "        x = embed[word.lower()]\n",
    "    elif corrected in embed: # last case, check if closest correction exists (might be bad, some corrections are kinda ass)\n",
    "        x = embed[corrected]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "Converts the lists for comments, titles into ndarrays\n",
    "\n",
    "@params : straightforward\n",
    "@returns: [comment_array,title_array] list of ndarrays for comments and titles\n",
    "\"\"\"\n",
    "def to_array(embed, comments, titles, max_comment_len, max_title_len):\n",
    "    comment_array = np.zeros((len(comments), max_comment_len, 300))\n",
    "    title_array = np.zeros((len(titles), max_title_len, 300))\n",
    "    for ix1, sent in enumerate(comments):\n",
    "        for ix2, word in enumerate(sent):\n",
    "            comment_array[ix1,ix2] = get_embed(word,embed)\n",
    "    for ix1, title in enumerate(titles):\n",
    "        for ix2, word in enumerate(title):\n",
    "            title_array[ix1,ix2] = get_embed(word,embed)\n",
    "    \n",
    "    return comment_array, title_array\n",
    "\n",
    "\"\"\"\n",
    "Randomly shuffles the outputs of to_array\n",
    "\"\"\"\n",
    "def custom_shuffle(comments,titles,labels):\n",
    "    \"\"\"\n",
    "    comments/title is a (batch_size, max_comment/title_length,embedding size) ndarray, this means we need batch_first=true in nn.LSTM\n",
    "    comment_array.shape = (batch_size, max_comment_len, 300) #300 is wor2vec embedding size\n",
    "    labels.shape = (batch_size,)\n",
    "    \"\"\"\n",
    "    num_samples, _ , _ = comments.shape\n",
    "    shuffled_indices = np.random.permutation(num_samples) #return a permutation of the indices\n",
    "    \n",
    "    shuffled_comments = comments[shuffled_indices,:,:]\n",
    "    shuffled_titles = titles[shuffled_indices,:,:]\n",
    "    shuffled_labels = labels[shuffled_indices]\n",
    "\n",
    "    return (shuffled_comments, shuffled_titles, shuffled_labels)\n",
    "\n",
    "\n",
    "def process_data(embed):\n",
    "    train_lines = open(\"./Data/fox-news-comments.json\", \"r\").readlines() # original 2015 data\n",
    "    test_lines = open(\"./Data/modern_comments.json\", \"r\").readlines() # modern data\n",
    "\n",
    "    train_labels, train_comments, train_titles, train_max_len, train_max_title_len = clean(train_lines)\n",
    "    test_labels, test_comments, test_titles, test_max_len, test_max_title_len = clean(test_lines)\n",
    "\n",
    "    train_comment_array, train_title_array = to_array(embed, train_comments, train_titles, train_max_len, train_max_title_len)\n",
    "    test_comment_array, test_title_array = to_array(embed, test_comments, test_titles, test_max_len, test_max_title_len)\n",
    "\n",
    "    train_comment_array, train_title_array, train_labels = custom_shuffle(train_comment_array,train_title_array,train_labels)\n",
    "    test_comment_array, test_title_array, test_labels = custom_shuffle(test_comment_array, test_title_array, test_labels)\n",
    "\n",
    "    train_comment_array = np.float32(train_comment_array)\n",
    "    train_title_array = np.float32(train_title_array)\n",
    "\n",
    "    test_comment_array = np.float32(test_comment_array)\n",
    "    test_title_array = np.float32(test_title_array)\n",
    "\n",
    "    return {'train' : [train_comment_array, train_title_array, train_labels], 'test' : [test_comment_array, test_title_array, test_labels]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
