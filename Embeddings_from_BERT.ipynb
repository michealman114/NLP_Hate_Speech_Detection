{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings from BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michealman114/Natural-Language-Models-for-Hate-Speech-Classification/blob/main/Embeddings_from_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TODO: Decrease embedding size from BERT, currently is at least 768\n",
        "TODO: Further improve embedding choice - either: sum last 4 hidden layers OR concatenate last 4 hidden layers\n",
        "TODO: Switch from DistilBert to Bert?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tTP2GSLRIiqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6BwGO3XFG-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a720900e-ead9-4a4b-e41e-bf0b9971ec3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "c7F2A4b6FaYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "if cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    seed = 4814\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    print(\"running on GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    print(\"running on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKGlhqr_U7QU",
        "outputId": "59b26c69-bd49-4845-83d5-285d10ad851a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on GPU: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getCommentsTitlesLabels(file_lines):\n",
        "    comment_list = []\n",
        "    title_list = []\n",
        "    labels = []\n",
        "    for line in file_lines:\n",
        "        content = json.loads(line)\n",
        "\n",
        "        comment = content['text']\n",
        "        comment_list.append(comment)\n",
        "\n",
        "        title = content['title']\n",
        "        title_list.append(title)\n",
        "\n",
        "        labels.append(content['label'])\n",
        "    \n",
        "    return comment_list,title_list,labels\n",
        "\n",
        "def custom_shuffle(comments_embeddings,titles_embeddings,labels):\n",
        "    \"\"\"\n",
        "    comments/title is a (batch_size, max_comment/title_length,embedding size) ndarray, this means we need batch_first=true in nn.LSTM\n",
        "    comment_array.shape = (batch_size, max_comment_len, 300) #300 is wor2vec embedding size\n",
        "    labels.shape = (batch_size,)\n",
        "    \"\"\"\n",
        "    num_samples, _ , _ = comments_embeddings.shape\n",
        "    shuffled_indices = np.random.permutation(num_samples) #return a permutation of the indices\n",
        "    \n",
        "    shuffled_comments = comments_embeddings[shuffled_indices,:,:]\n",
        "    shuffled_titles = titles_embeddings[shuffled_indices,:,:]\n",
        "    shuffled_labels = labels[shuffled_indices]\n",
        "\n",
        "    return (shuffled_comments, shuffled_titles, shuffled_labels)"
      ],
      "metadata": {
        "id": "qg5GItyeST6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased', config=config).to(device)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcRhQJE2OsgC",
        "outputId": "a1b08a24-ee95-41a5-ec0b-c97882caf3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_lines = open(\"./Data/modern_comments.json\", \"r\").readlines() # modern data\n",
        "#test_comments,test_titles,test_labels = getCommentsTitlesLabels(test_lines)\n",
        "#test_labels = torch.tensor(test_labels)\n",
        "\n",
        "#tokenized_test_comments = tokenizer(train_comments, padding = True, truncation = True, return_tensors=\"pt\").to(device)\n",
        "#tokenized_test_titles = tokenizer(train_titles, padding = True, truncation = True, return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "id": "FvuC9qW0ayUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = open(\"./Data/fox-news-comments.json\", \"r\").readlines() # original 2015 data\n",
        "train_comments, train_titles, train_labels = getCommentsTitlesLabels(train_lines)\n",
        "train_labels = torch.tensor(train_labels)"
      ],
      "metadata": {
        "id": "C10afAGQFoyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6gjARAjcMsa",
        "outputId": "208b37d1-a0d0-40d0-dd5c-de2f304e76ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1528"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CommentsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, comments, titles, labels):\n",
        "        \"\"\"\n",
        "        comments/titles: (batch_size, max_length, embed_dim)\n",
        "        labels: (batch_size,)\n",
        "        \"\"\"\n",
        "        #Initialization\n",
        "        self.comments = comments\n",
        "        self.titles = titles\n",
        "        self.labels = labels\n",
        "        self.length = labels.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        comment = self.comments[index]\n",
        "        title = self.titles[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return comment,title,label"
      ],
      "metadata": {
        "id": "5hYoJd4ibafo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = CommentsDataset(train_comments, train_titles, train_labels)\n",
        "train_loader = torch_data.DataLoader(train_data, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "S0wUvg22cpbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_comments[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-1mpYxCeBAZ",
        "outputId": "781b3e8e-a36d-4fc2-a4b7-4881a553bb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Merkel would never say NO', 'Expect more and more women to be asking .. \"why are men no longer interested in me\"! We\\'re not going touch you until you pull our pants down!', \"Groping people in public wasn't already illegal? What's up with that, Deutschland?\", 'Merkel, possible the only person in charge who is worse than what we have. Obama is trying his hardest though to get to Merkel\\'s level. \"A 21-year-old Iraqi man was convicted of sexual assault and given a one-year suspended sentence. A 26-year-old Algerian man was convicted of abetting a sexual assault and attempted assault, and given the same sentence.\" Sounds exactly like how Obama is trying to let all of the black people in this country get away with everything.', 'They know very well, no means NO! They need to pass a law making it legal to castrate those animals.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the model in eval mode to turn off dropout regularization etc.\n",
        "model.eval()\n",
        "\n",
        "embedded_train_comments = []\n",
        "embedded_train_titles = []\n",
        "embedded_train_labels = []\n",
        "\n",
        "i = 1\n",
        "\n",
        "#use torch.no_grad() to speed up the embedding process\n",
        "with torch.no_grad():\n",
        "    for comments, titles, labels in train_loader:\n",
        "        \"\"\"\n",
        "        comments: list of size (num_comments)\n",
        "        titles: list of size (num_comments)\n",
        "        labels: torch.tensor of size (num_comments)\n",
        "        \"\"\"\n",
        "\n",
        "        tokenized_train_comments = tokenizer(comments, padding = \"max_length\", max_length = 512, truncation = True, return_tensors=\"pt\").to(device)\n",
        "        tokenized_train_titles = tokenizer(titles, padding = \"max_length\", max_length = 512, truncation = True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "        train_comments_outputs = model(**tokenized_train_comments) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
        "        train_titles_outputs = model(**tokenized_train_titles) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
        "\n",
        "        all_train_comments_embeddings = train_comments_outputs[1] #tuple of hidden states from each layer of DistilBERT\n",
        "        all_train_titles_embeddings = train_titles_outputs[1] #tuple of hidden states from each layer of DistilBERT\n",
        "\n",
        "        final_train_comments_embeddings_temp = all_train_comments_embeddings[-2]\n",
        "        final_train_titles_embeddings_temp = all_train_titles_embeddings[-2]\n",
        "\n",
        "        #print(final_train_comments_embeddings_temp.shape) #(batch_size=128, length = 512, embed_dim = 768)\n",
        "\n",
        "        train_comments.append(final_train_comments_embeddings_temp)\n",
        "        embedded_train_comments.append(final_train_comments_embeddings_temp)\n",
        "        embedded_train_titles.append(final_train_titles_embeddings_temp)\n",
        "        embedded_train_labels.append(labels)\n",
        "\n",
        "        print(f'iteration {i} completed')\n",
        "        i+= 1\n",
        "\n",
        "\n",
        "\n",
        "final_train_comments_embeddings = torch.cat(embedded_train_comments)\n",
        "final_train_titles_embeddings = torch.cat(embedded_train_titles)\n",
        "\n",
        "print(final_train_comments_embeddings.shape)\n",
        "print(final_train_titles_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQkKSrHYdGxt",
        "outputId": "6dd7c244-6459-4bb3-d5b0-b8400da6474c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1 completed\n",
            "iteration 2 completed\n",
            "iteration 3 completed\n",
            "iteration 4 completed\n",
            "iteration 5 completed\n",
            "iteration 6 completed\n",
            "iteration 7 completed\n",
            "iteration 8 completed\n",
            "iteration 9 completed\n",
            "iteration 10 completed\n",
            "iteration 11 completed\n",
            "iteration 12 completed\n",
            "torch.Size([1528, 512, 768])\n",
            "torch.Size([1528, 512, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_train_comments = tokenizer(train_comments, padding = True, truncation = True, return_tensors=\"pt\")\n",
        "tokenized_train_titles = tokenizer(train_titles, padding = True, truncation = True, return_tensors=\"pt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sLLSLmAebUzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_train_comments['input_ids'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehhNnHbsGkIn",
        "outputId": "5d58f319-57ad-441a-b207-fe20bc0c7f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1528, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the model in eval mode to turn off dropout regularization etc.\n",
        "model.eval()\n",
        "\n",
        "\n",
        "#use torch.no_grad() to speed up the embedding process\n",
        "with torch.no_grad():\n",
        "    train_comments_outputs = model(**tokenized_train_comments) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
        "    last_train_comments_embeddings = train_comments_outputs[0]\n",
        "    all_train_comments_embeddings = train_comments_outputs[1] \n",
        "    # comments_embeddings is a tuple of hidden states at each layer of DistilBERT\n",
        "    # (comments_embeddings[0] corresponds to first layer, comments_embeddings[6] corresponds to last hidden layer)\n",
        "\n",
        "    train_titles_outputs = model(**tokenized_train_titles) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
        "    last_train_titles_embeddings = train_titles_outputs[0]\n",
        "    all_train_titles_embeddings = train_titles_outputs[1] \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    test_comments_outputs = model(**tokenized_test_comments) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
        "    last_test_comments_embeddings = test_comments_outputs[0]\n",
        "    all_test_comments_embeddings = test_comments_outputs[1]\n",
        "\n",
        "    test_titles_outputs = model(**tokenized_test_titles) # (last_hidden_state,hidden_states[optional], attentions[optional])\n",
        "    last_test_titles_embeddings = test_titles_outputs[0]\n",
        "    all_test_titles_embeddings = test_titles_outputs[1] \n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "UNAZlN0hI_44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(last_train_comments_embeddings[:1])\n",
        "print(all_train_comments_embeddings[6][:1])"
      ],
      "metadata": {
        "id": "7dJnH-qAJuho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: produce best embeddings by summing the last 4 layers (for DistilBert vs Bert this would probably be smaller)\n",
        "#Ref: http://jalammar.github.io/illustrated-bert/ https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca\n",
        "\n",
        "final_train_comments_embeddings = all_train_comments_embeddings[5]\n",
        "final_train_titles_embeddings = all_train_titles_embeddings[5]\n",
        "print(final_train_comments_embeddings.shape)\n",
        "print(final_train_titles_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN4vUhL3Q0rz",
        "outputId": "c6aaa1fc-808b-48d6-c877-d1356c0fa46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 106, 768])\n",
            "torch.Size([5, 16, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_train_comments_embeddings, final_train_titles_embeddings, train_labels = custom_shuffle(final_train_comments_embeddings, final_train_titles_embeddings, train_labels)"
      ],
      "metadata": {
        "id": "CV9IZJQsRo-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Svir-DfJR2w9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}