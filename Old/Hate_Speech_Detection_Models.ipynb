{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michealman114/NLP_Hate_Speech_Detection/blob/main/Hate_Speech_Detection_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmH07IZt6cKq",
    "outputId": "e95075e7-3b46-46cf-831d-371d8ebfe9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=ac4afc512847715e94ef1df29a997b7afc70e479886ac36a1fc5a25aa445d7b1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 4.6 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.6.3\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install textblob\n",
    "!pip install pyspellchecker\n",
    "!pip install nltk\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDsyiKCAls3P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFKyzVpTKX6e",
    "outputId": "321b91ad-15a6-42dd-87b6-628d94d41013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wget\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob \n",
    "import re\n",
    "\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "jeTOv8zGlCj8",
    "outputId": "a3b8b946-7f16-43e3-e57f-a09d0ecce9c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "\n",
    "seed = 4814\n",
    "\n",
    "if cuda.is_available():\n",
    "  device = 'cuda'\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "else:\n",
    "  print('running on cpu')\n",
    "  device = 'cpu'\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZ8Ur2xM6mrk",
    "outputId": "93ecbe29-8949-42b5-f526-733c5f0b4326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox-news-comments.json\n",
      "modern_comments.json\n"
     ]
    }
   ],
   "source": [
    "data = wget.download('https://raw.githubusercontent.com/sjtuprog/fox-news-comments/master/fox-news-comments.json')\n",
    "\n",
    "modern_data = wget.download('https://raw.githubusercontent.com/michealman114/NLP_Hate_Speech_Detection/main/modern_comments.json')\n",
    "\n",
    "print(data)\n",
    "print(modern_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MT1S5zgF6vgL",
    "outputId": "476535e2-5817-48b9-fc14-0f9263e393bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox-news-comments.json\tmodern_comments.json  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2klHVK66zIm"
   },
   "outputs": [],
   "source": [
    "train_lines = open(\"fox-news-comments.json\", \"r\").readlines()\n",
    "test_lines = open(\"modern_comments.json\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Pwa9qQ4OiVv"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('')\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMNRZcnNscRz"
   },
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "  max_len = 0\n",
    "  max_title_len = 0  \n",
    "  text_list = []\n",
    "  title_list = []\n",
    "  label = []\n",
    "  for i in data:\n",
    "    comment = json.loads(i)\n",
    "    t = comment['text']\n",
    "    t = ' '.join([x for x in t.split() if x[0] != '@'])\n",
    "    t = ' '.join(re.findall(\"[a-zA-Z,.]+\",t))\n",
    "    t = t.replace(',', ' ')\n",
    "    t = t.replace('.', ' ')\n",
    "    text = word_tokenize(t)\n",
    "    text = [x for x in text if x.lower() not in stop_words]\n",
    "    max_len = max(max_len, len(text))\n",
    "    text_list.append(text)\n",
    "    title = comment['title']\n",
    "    title = title.replace(',', '')\n",
    "    title = title.replace('.', '')\n",
    "    title = re.findall(\"[a-zA-Z,.]+\",title)\n",
    "    title_list.append(title)\n",
    "    max_title_len = max(max_title_len, len(title))\n",
    "    label.append(comment['label'])\n",
    "  \n",
    "  labels = np.array(label)\n",
    "  return text_list, labels, title_list, max_len, max_title_len\n",
    "\n",
    "train_text, train_labels, train_title, train_max_len, train_max_title_len = clean(train_lines)\n",
    "test_text, test_labels, test_title, test_max_len, test_max_title_len = clean(test_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kt78nM8nn9u"
   },
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "spell.word_frequency.add('obama')\n",
    "spell.word_frequency.add('blm')\n",
    "spell.word_frequency.add('killing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cScKHN-oDgub",
    "outputId": "430829ba-1979-45d1-a4ad-800dc3b7e54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FB1YAgRK3P-q"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "embed = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_A2zjwuEiM7"
   },
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "  x = np.zeros((300,)) # default value should be 0\n",
    "  corrected = spell.correction(word) # closest correction\n",
    "  if word in embed: # base word\n",
    "    x = embed[word]\n",
    "  elif word.upper() in embed: # capitalized (edge case for acronyms like BLM - blm doesn't exist but BLM does)\n",
    "    x = embed[word.upper()]\n",
    "  elif word.lower() in embed: # opposite of capitalization\n",
    "    x = embed[word.lower()]\n",
    "  elif corrected in embed: # last case, check if closest correction exists (some corrections are not great)\n",
    "    x = embed[corrected]\n",
    "  \n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnBoilj3_a-c",
    "outputId": "490d7a72-b799-48c4-bec4-c8cc3b950905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments 244\n",
      "title 13\n",
      "1528\n",
      "1528\n",
      "(1528,)\n"
     ]
    }
   ],
   "source": [
    "print('comments',train_max_len)\n",
    "print('title',train_max_title_len)\n",
    "print(len(train_text))\n",
    "print(len(train_title))\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ePOuiaUt6NJ",
    "outputId": "39daac90-4e6f-4d93-b836-2c5fb67e5a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_comments 126\n",
      "test_title 19\n",
      "102\n",
      "102\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "print('test_comments', test_max_len)\n",
    "print('test_title', test_max_title_len)\n",
    "print(len(test_text))\n",
    "print(len(test_title))\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XS_7b8wuH1B3"
   },
   "outputs": [],
   "source": [
    "def to_array(comments, titles, max_comment_len, max_title_len):\n",
    "  data_array = np.zeros((max_comment_len, len(comments), 300))\n",
    "  title_array = np.zeros((max_title_len, len(titles), 300))\n",
    "  for ix1, sent in enumerate(comments):\n",
    "    for ix2, word in enumerate(sent):\n",
    "      data_array[ix2,ix1] = get_embed(word)\n",
    "  for ix1, title in enumerate(titles):\n",
    "    for ix2, word in enumerate(title):\n",
    "      title_array[ix2,ix1] = get_embed(word)\n",
    "  \n",
    "  return data_array, title_array\n",
    "\n",
    "train_data_array, train_title_array = to_array(train_text, train_title, train_max_len, train_max_title_len)\n",
    "test_data_array, test_title_array = to_array(test_text, test_title, test_max_len, test_max_title_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLtn7pMsNxuT",
    "outputId": "00f2cee6-e5a8-44ce-8d55-b959e90df66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 1528, 300) (13, 1528, 300)\n",
      "(126, 102, 300) (19, 102, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_array.shape, train_title_array.shape)\n",
    "print(test_data_array.shape, test_title_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5V9MKCgAfMQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_array.shape = (244, 1528, 300)\n",
    "data_labels.shape = (1528,)\n",
    "data is an (L,N,D) array\n",
    "L = max_length of sequence\n",
    "N = batch_size\n",
    "D = embed_dim\n",
    "\"\"\"\n",
    "\n",
    "#Source: https://likegeeks.com/numpy-shuffle/#Shuffle_multiple_NumPy_arrays_together\n",
    "def custom_shuffle(data,titles,labels):\n",
    "    _, num_samples, _ = data.shape\n",
    "    shuffled_indices = np.random.permutation(num_samples) #return a permutation of the indices\n",
    "    new_data = data[:,shuffled_indices,:]\n",
    "    new_labels = labels[shuffled_indices]\n",
    "    new_titles = titles[:,shuffled_indices,:]\n",
    "\n",
    "    return (new_data, new_titles, new_labels)\n",
    "    \n",
    "train_data_array,train_title_array,train_labels = custom_shuffle(train_data_array,train_title_array,train_labels)\n",
    "\n",
    "test_data_array, test_title_array, test_labels = custom_shuffle(test_data_array, test_title_array, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwKhLRLjjRlp",
    "outputId": "7f38e8d1-4e9c-4831-f031-de2aed393a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_array[:,0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuBKMoJAb6Bm",
    "outputId": "d6d77f4c-587b-4833-de88-ea5ce101a419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(train_data_array.dtype)\n",
    "print(type(train_data_array))\n",
    "print(train_title_array.dtype)\n",
    "print(type(train_title_array))\n",
    "\n",
    "data_array = np.float32(train_data_array)\n",
    "title_array = np.float32(train_title_array)\n",
    "\n",
    "test_data_array = np.float32(test_data_array)\n",
    "test_title_array = np.float32(test_title_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmsQ7LWLwJKF"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module): # single direction lstm, no attention\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300):\n",
    "    super(BaseModel, self).__init__()\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "    self.linear1 = nn.Linear(hidden_size, 150) # map context vector to value\n",
    "    self.linear2 = nn.Linear(150, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = False, dropout = 0.2, bidirectional = False)\n",
    "\n",
    "  def forward(self, data):\n",
    "    \"\"\"\n",
    "    data is an (L,N,D) array\n",
    "    L = max_length of sequence\n",
    "    N = batch_size\n",
    "    D = embed_dim\n",
    "    returns an (N,1) array of probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm(data) # (L,N,H) array\n",
    "    \n",
    "    sentences = torch.sum(hidden_states, axis = 0)\n",
    "\n",
    "    return self.sigmoid(torch.squeeze(self.linear2(self.relu(self.linear1(sentences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el3YmujixdaI"
   },
   "outputs": [],
   "source": [
    "class BidiModel(nn.Module): # Bidi\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "    self.linear1 = nn.Linear(2*hidden_size, hidden_size) # map context vector to value\n",
    "    self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = False, dropout = 0.2, bidirectional = True)\n",
    "\n",
    "  def forward(self, data):\n",
    "    \"\"\"\n",
    "    data is an (L,N,D) array\n",
    "    L = max_length of sequence\n",
    "    N = batch_size\n",
    "    D = embed_dim\n",
    "    returns an (N,1) array of probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm(data) # (L,N,2H) array\n",
    "\n",
    "    sentences = torch.sum(hidden_states, axis = 0)\n",
    "\n",
    "    return self.sigmoid(torch.squeeze(self.linear2(self.relu(self.linear1(sentences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8Q1ParoECkr"
   },
   "outputs": [],
   "source": [
    "class FullModel(nn.Module): # bidi with attention\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    #self.embedding = embed\n",
    "    \n",
    "    self.linear1 = nn.Linear(2*hidden_size, hidden_size) # map context vector to value\n",
    "    self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    self.attention1 = nn.Linear(2*hidden_size, 50) # map hidden state vector to value\n",
    "    self.attention2 = nn.Linear(50, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.sm = nn.Softmax(dim = 0)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = False, dropout = 0.2, bidirectional = True)\n",
    "\n",
    "  def forward(self, data):\n",
    "    \"\"\"\n",
    "    data is an (L,N,D) array\n",
    "    L = max_length of sequence\n",
    "    N = batch_size\n",
    "    D = embed_dim\n",
    "    returns an (N,1) array of probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm(data) # (L,N,2H) array\n",
    "    weights = self.attention2(self.relu(self.attention1(hidden_states))) # (L,N,1) array\n",
    "    \n",
    "    alpha = self.sm(weights.reshape(weights.shape[:-1])) # weights\n",
    "\n",
    "    hidden_states = torch.moveaxis(hidden_states, -1, 0)\n",
    "\n",
    "\n",
    "    sentences = torch.sum(hidden_states * alpha, axis = 1)\n",
    "\n",
    "    sentences = torch.moveaxis(sentences, 0, -1)\n",
    "\n",
    "    return self.sigmoid(torch.squeeze(self.linear2(self.relu(self.linear1(sentences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ud5Q_0ORk9T"
   },
   "outputs": [],
   "source": [
    "class ModelWithTitle(nn.Module): # bidi with attention\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "    self.linear1 = nn.Linear(4*hidden_size, hidden_size) # map context vector to value (concatenated from parallel networks)\n",
    "    self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.sm = nn.Softmax(dim = 0)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    #comments\n",
    "    self.attention1_comment = nn.Linear(2*hidden_size, 50) # map hidden state vector to value\n",
    "    self.attention2_comment = nn.Linear(50, 1)\n",
    "    \n",
    "    self.lstm_comment = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = False, dropout = 0.2, bidirectional = True)\n",
    "\n",
    "    #titles\n",
    "    self.attention1_title = nn.Linear(2*hidden_size, 50)\n",
    "    self.attention2_title = nn.Linear(50, 1)\n",
    "\n",
    "    self.lstm_title = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = False, dropout = 0.2, bidirectional = True)\n",
    "\n",
    "  def forward(self, comment_data, title_data):\n",
    "    \"\"\"\n",
    "    comments is an (L1,N,D) array\n",
    "    titles is an (L2,N,D) array\n",
    "    L1 = max_length of sequence\n",
    "    L2 = max_length of title\n",
    "    N = batch_size\n",
    "    D = embed_dim\n",
    "    returns an (N,1) array of probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm_comment(comment_data) # (L,N,2H) array\n",
    "\n",
    "    weights = self.attention2_comment(self.relu(self.attention1_comment(hidden_states))) # (L,N,1) array\n",
    "    \n",
    "    alpha = self.sm(weights.reshape(weights.shape[:-1])) # weights\n",
    "\n",
    "    hidden_states = torch.moveaxis(hidden_states, -1, 0)\n",
    "\n",
    "    sentences = torch.sum(hidden_states * alpha, axis = 1)\n",
    "\n",
    "    sentences = torch.moveaxis(sentences, 0, -1) # sentences is N x 2*hidden_size\n",
    "\n",
    "    hidden_states, (_,_) = self.lstm_title(title_data)\n",
    "    weights = self.attention2_title(self.relu(self.attention1_title(hidden_states)))\n",
    "    alpha = self.sm(weights.reshape(weights.shape[:-1]))\n",
    "    hidden_states = torch.moveaxis(hidden_states, -1, 0)\n",
    "    titles = torch.sum(hidden_states * alpha, axis = 1)\n",
    "    titles = torch.moveaxis(titles, 0, -1) # titles is N x 2*hidden_size\n",
    "\n",
    "    result = torch.cat((sentences, titles), dim = 1)\n",
    "\n",
    "    return self.sigmoid(torch.squeeze(self.linear2(self.relu(self.linear1(result)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2uR83ggin0N"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, list_IDs, data, labels, titles = None):\n",
    "        # initialization now works with titles, passes in optional title information\n",
    "        # works the same as before, but now gets title data if you give it to it\n",
    "        'Initialization'\n",
    "        self.data = data\n",
    "        self.titles = titles\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.data[:,ID,:]\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        if self.titles is not None:\n",
    "          t = self.titles[:,ID,:]\n",
    "          return X, t, y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNV-Ri86E8n4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(data, labels, n_epochs, batch_size, modeltype, model = None):\n",
    "    device = torch.device('cuda')  # run on colab gpu\n",
    "\n",
    "    if model is None:\n",
    "        model = modeltype().to(device)\n",
    "        \n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    training_data = Dataset(range(len(labels)), data, labels)\n",
    "\n",
    "    loader = torch_data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for context, label in loader:\n",
    "            context = context.to(device)\n",
    "            context = context.moveaxis(0, 1)\n",
    "            label = label.to(device).type(torch.float32)\n",
    "\n",
    "            preds = model.forward(context)\n",
    "            #print(preds)\n",
    "            #print(label)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(preds, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print('Loss:', epoch_loss)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "    print(losses)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP_UglQ9W7JQ"
   },
   "outputs": [],
   "source": [
    "def train_with_titles(data, titles, labels, n_epochs, batch_size, model = None):\n",
    "    device = torch.device('cuda')  # run on colab gpu\n",
    "\n",
    "    if model is None:\n",
    "        model = ModelWithTitle().to(device)\n",
    "        \n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    training_data = Dataset(range(len(labels)), data, labels, titles = titles)\n",
    "\n",
    "    loader = torch_data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for context, t, label in loader:\n",
    "            context = context.to(device)\n",
    "            context = context.moveaxis(0, 1)\n",
    "            t = t.to(device)\n",
    "            t = t.moveaxis(0,1)\n",
    "            label = label.to(device).type(torch.float32)\n",
    "\n",
    "            preds = model.forward(context, t)\n",
    "            #print(preds)\n",
    "            #print(label)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(preds, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print('Loss:', epoch_loss)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "    print(losses)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_H1Ezexkg45"
   },
   "outputs": [],
   "source": [
    "print(data_array.shape)\n",
    "model = train_with_titles(train_data_array, train_title_array, train_labels, 30, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUto5MLrv5dA"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WP_hAengFCTn"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_array.shape = (244, 1528, 300)\n",
    "data_labels.shape = (1528,)\n",
    "data is an (L,N,D) array\n",
    "L = max_length of sequence\n",
    "N = batch_size\n",
    "D = embed_dim\n",
    "\"\"\"\n",
    "\n",
    "def kfold_crossvalidation(data, labels, modeltype, k, n_epochs = 30, model=None):\n",
    "    _, num_samples, _ = data.shape\n",
    "    fraction = 1/k\n",
    "    seg = int(num_samples * fraction)\n",
    "    segment_indices = []\n",
    "    for i in range(k):\n",
    "        vall = i * seg\n",
    "        valr = i * seg + seg\n",
    "        segment_indices.append(list(range(vall,valr)))\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    #actually run the ith split\n",
    "    for i in range(k):\n",
    "        train_indices = []\n",
    "        test_indices = segment_indices[i]\n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                train_indices.extend(segment_indices[j])\n",
    "\n",
    "    \n",
    "        train_data = data[:,train_indices,:]\n",
    "        train_labels = labels[train_indices]\n",
    "\n",
    "        test_data = data[:,test_indices,:]\n",
    "        test_labels = labels[test_indices]\n",
    "\n",
    "        batch_size = 128\n",
    "        model_i = train(train_data, train_labels, n_epochs, batch_size, modeltype)\n",
    "\n",
    "        iter_loss, (y_pred, y_true) = test_model(model_i,test_data,test_labels)\n",
    "        y_pred = torch.round(y_pred).cpu().detach().numpy()\n",
    "        all_preds.append(y_pred)\n",
    "        all_labels.append(y_true)\n",
    "        #print(type(y_true), y_true.shape)\n",
    "        #print(type(y_pred), y_pred.shape)\n",
    "        print('Accuracy:',accuracy_score(y_true,y_pred))\n",
    "        print('Precision, Recall, F1:',precision_recall_fscore_support(y_true, y_pred, average='binary'))\n",
    "    \n",
    "    print('\\n===Aggregate Stats===')\n",
    "    p = np.concatenate(all_preds, axis = None)\n",
    "    l = np.concatenate(all_labels, axis = None)\n",
    "    print('Accuracy:', accuracy_score(l, p))\n",
    "    print('Precision, Recall, F1:', precision_recall_fscore_support(l, p, average = 'binary'))\n",
    "\n",
    "def test_model(model, test_data, test_labels):\n",
    "    test_dataset = Dataset(range(len(test_labels)), test_data, test_labels)\n",
    "\n",
    "    test_loader = torch_data.DataLoader(test_dataset, batch_size=len(test_labels))\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    predictions = None\n",
    "    \n",
    "    for context, label in test_loader:\n",
    "        context = context.to(device)\n",
    "        context = context.moveaxis(0, 1)\n",
    "        label = label.to(device).type(torch.float32)\n",
    "\n",
    "        #preds is a tensor of roughly torch.Size([305])\n",
    "        preds = model.forward(context)\n",
    "        predictions = preds\n",
    "\n",
    "        loss = loss_fn(preds, label)\n",
    "\n",
    "        print(loss.item())\n",
    "    \n",
    "\n",
    "    return loss.item(), (preds, test_labels)\n",
    "\n",
    "\n",
    "def test_split():\n",
    "    #function to verify that the function was splitting the data correctly\n",
    "    temp_data = np.random.rand(2,5,3)\n",
    "    temp_labels = np.random.randint(2, size=5)\n",
    "    print(\"data:\\n\", temp_data)\n",
    "    print(\"labels:\", temp_labels)\n",
    "\n",
    "    kfold_crossvalidation(temp_data,temp_labels,10)\n",
    "\n",
    "kfold_crossvalidation(train_data_array,train_labels, modeltype = FullModel, k = 10,n_epochs = 30, model = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs3_s7__YRvs"
   },
   "outputs": [],
   "source": [
    "def kfold_crossvalidation_titles(data, titles, labels, k, n_epochs = 30, model=None):\n",
    "    _, num_samples, _ = data.shape\n",
    "    fraction = 1/k\n",
    "    seg = int(num_samples * fraction)\n",
    "    segment_indices = []\n",
    "    for i in range(k):\n",
    "        vall = i * seg\n",
    "        valr = i * seg + seg\n",
    "        segment_indices.append(list(range(vall,valr)))\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    #actually run the ith split\n",
    "    for i in range(k):\n",
    "        train_indices = []\n",
    "        test_indices = segment_indices[i]\n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                train_indices.extend(segment_indices[j])\n",
    "\n",
    "    \n",
    "        train_data = data[:,train_indices,:]\n",
    "        train_titles = titles[:,train_indices,:]\n",
    "        train_labels = labels[train_indices]\n",
    "\n",
    "        test_data = data[:,test_indices,:]\n",
    "        test_titles = titles[:,test_indices,:]\n",
    "        test_labels = labels[test_indices]\n",
    "\n",
    "        batch_size = 128\n",
    "        model_i = train_with_titles(train_data, train_titles, train_labels, n_epochs, batch_size)\n",
    "\n",
    "        iter_loss, (y_pred, y_true) = test_model_titles(model_i,test_data,test_titles,test_labels)\n",
    "        y_pred = torch.round(y_pred).cpu().detach().numpy()\n",
    "        all_preds.append(y_pred)\n",
    "        all_labels.append(y_true)\n",
    "        #print(type(y_true), y_true.shape)\n",
    "        #print(type(y_pred), y_pred.shape)\n",
    "        print('Accuracy:',accuracy_score(y_true,y_pred))\n",
    "        print('Precision, Recall, F1:',precision_recall_fscore_support(y_true, y_pred, average='binary'))\n",
    "    \n",
    "    print('\\n===Aggregate Stats===')\n",
    "    p = np.concatenate(all_preds, axis = None)\n",
    "    l = np.concatenate(all_labels, axis = None)\n",
    "    print('Accuracy:', accuracy_score(l, p))\n",
    "    print('Precision, Recall, F1:', precision_recall_fscore_support(l, p, average = 'binary'))\n",
    "\n",
    "def test_model_titles(model, test_data, test_titles, test_labels):\n",
    "    test_dataset = Dataset(range(len(test_labels)), test_data, test_labels, titles = test_titles)\n",
    "\n",
    "    test_loader = torch_data.DataLoader(test_dataset, batch_size=len(test_labels))\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    predictions = None\n",
    "    \n",
    "    for context, t, label in test_loader:\n",
    "        context = context.to(device)\n",
    "        context = context.moveaxis(0, 1)\n",
    "        t = t.to(device)\n",
    "        t = t.moveaxis(0,1)\n",
    "        label = label.to(device).type(torch.float32)\n",
    "\n",
    "        #preds is a tensor of roughly torch.Size([305])\n",
    "        preds = model.forward(context,t)\n",
    "        predictions = preds\n",
    "\n",
    "        loss = loss_fn(preds, label)\n",
    "\n",
    "        print(loss.item())\n",
    "    \n",
    "\n",
    "    return loss.item(), (preds, test_labels)\n",
    "\n",
    "kfold_crossvalidation_titles(train_data_array,train_title_array,train_labels,10,n_epochs = 40, model = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDuvBrSRI_2m"
   },
   "outputs": [],
   "source": [
    "trained_model = train(train_data_array, train_labels, 20, 128, modeltype = FullModel)\n",
    "_, (p, l) = test_model(trained_model, test_data_array, test_labels)\n",
    "p = torch.round(p).cpu().detach().numpy()\n",
    "print('Accuracy:',accuracy_score(l,p))\n",
    "print('Precision, Recall, F1:',precision_recall_fscore_support(l, p, average = 'binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5nrceqlk58F"
   },
   "outputs": [],
   "source": [
    "trained_model = train_with_titles(train_data_array, train_title_array, train_labels, 20, 128)\n",
    "_, (p, l) = test_model_titles(trained_model, test_data_array, test_title_array, test_labels)\n",
    "p = torch.round(p).cpu().detach().numpy()\n",
    "print('Accuracy:',accuracy_score(l,p))\n",
    "print('Precision, Recall, F1:',precision_recall_fscore_support(l, p, average = 'binary'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of 6.864 final project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
