{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e64510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787fbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8474c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anugrahchemparathy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anugrahchemparathy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anugrahchemparathy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/anugrahchemparathy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4842ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "spell.word_frequency.add('obama')\n",
    "spell.word_frequency.add('blm')\n",
    "spell.word_frequency.add('killing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc44ed98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m process_data\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "  \n",
    "# setting path\n",
    "sys.path.append('../parentdirectory')\n",
    "\n",
    "from ... import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00401b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleans the dataset and returns the \n",
    "\n",
    "@param file_lines: list of lines in the input file where each line contains all the information for a given comment (content + title + author title + etc.)\n",
    "\n",
    "@returns [labels, comment_list, title_list, max_len, max_title_len]\n",
    "    labels: file\n",
    "    comment_list: list of all comments in the file\n",
    "    title_list: list of all titles in the file\n",
    "    max_comment_len: length of the longest comment in the dataset\n",
    "    max_title_len: length of the longest title in the dataset\n",
    "\"\"\"\n",
    "def clean(file_lines):\n",
    "    max_len = 0\n",
    "    max_title_len = 0  \n",
    "    comment_list = []\n",
    "    title_list = []\n",
    "    label = []\n",
    "    for line in file_lines:\n",
    "        comment = json.loads(line)\n",
    "        \n",
    "        t = comment['text']\n",
    "        t = ' '.join([x for x in t.split() if x[0] != '@'])\n",
    "        t = ' '.join(re.findall(\"[a-zA-Z,.]+\",t))\n",
    "        t = t.replace(',', ' ')\n",
    "        t = t.replace('.', ' ')\n",
    "        text = word_tokenize(t)\n",
    "        text = [x for x in text if x.lower() not in stop_words]\n",
    "        max_len = max(max_len, len(text))\n",
    "        comment_list.append(text)\n",
    "        \n",
    "        title = comment['title']\n",
    "        title = title.replace(',', '')\n",
    "        title = title.replace('.', '')\n",
    "        title = re.findall(\"[a-zA-Z,.]+\",title)\n",
    "        title_list.append(title)\n",
    "        max_title_len = max(max_title_len, len(title))\n",
    "        \n",
    "        label.append(comment['label'])\n",
    "    \n",
    "    labels = np.array(label)\n",
    "    return labels, comment_list, title_list, max_len, max_title_len\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns word2vec embeddings for an input word string\n",
    "\n",
    "@param word : a string\n",
    "@param embed : the embedding keyed vectors (in our case word2vec)\n",
    "@returns : the (300,0) embedding for word\n",
    "\"\"\"\n",
    "def get_embed(word, embed):\n",
    "    x = np.zeros((300,)) # default value should be 0\n",
    "    corrected = spell.correction(word) # closest correction\n",
    "    if word in embed: # base word\n",
    "        x = embed[word]\n",
    "    elif word.upper() in embed: # capitalized (edge case for acronyms like BLM) (for some reason blm doesn't exist but BLM does?)\n",
    "        x = embed[word.upper()]\n",
    "    elif word.lower() in embed: # opposite of capitalization\n",
    "        x = embed[word.lower()]\n",
    "    elif corrected in embed: # last case, check if closest correction exists (might be bad, some corrections are kinda ass)\n",
    "        x = embed[corrected]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "Converts the lists for comments, titles into ndarrays\n",
    "\n",
    "@params : straightforward\n",
    "@returns: [comment_array,title_array] list of ndarrays for comments and titles\n",
    "\"\"\"\n",
    "def to_array(embed, comments, titles, max_comment_len, max_title_len):\n",
    "    comment_array = np.zeros((len(comments), max_comment_len, 300))\n",
    "    title_array = np.zeros((len(titles), max_title_len, 300))\n",
    "    for ix1, sent in enumerate(comments):\n",
    "        for ix2, word in enumerate(sent):\n",
    "            comment_array[ix1,ix2] = get_embed(word,embed)\n",
    "    for ix1, title in enumerate(titles):\n",
    "        for ix2, word in enumerate(title):\n",
    "            title_array[ix1,ix2] = get_embed(word,embed)\n",
    "    \n",
    "    return comment_array, title_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "from datasets import *\n",
    "from models import *\n",
    "from process_data import *\n",
    "\n",
    "\n",
    "\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "#print(path) #/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
    "\n",
    "embed = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "train_lines = open(\"./Data/fox-news-comments.json\", \"r\").readlines() #original 2015 data\n",
    "test_lines = open(\"./Data/modern_comments.json\", \"r\").readlines() #modern data\n",
    "\n",
    "train_labels, train_comments, train_titles, train_max_len, train_max_title_len = clean(train_lines)\n",
    "test_labels, test_comments, test_titles, test_max_len, test_max_title_len = clean(test_lines)\n",
    "\n",
    "train_comment_array, train_title_array = to_array(embed, train_comments, train_titles, train_max_len, train_max_title_len)\n",
    "test_comment_array, test_title_array = to_array(embed, test_comments, test_titles, test_max_len, test_max_title_len)\n",
    "\n",
    "train_comment_array,train_title_array,train_labels = custom_shuffle(train_comment_array,train_title_array,train_labels)\n",
    "test_comment_array, test_title_array, test_labels = custom_shuffle(test_comment_array, test_title_array, test_labels)\n",
    "\n",
    "train_comment_array = np.float32(train_comment_array)\n",
    "train_title_array = np.float32(train_title_array)\n",
    "\n",
    "test_comment_array = np.float32(test_comment_array)\n",
    "test_title_array = np.float32(test_title_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
