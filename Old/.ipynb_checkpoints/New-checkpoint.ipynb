{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('longet train comment length',train_max_len)\n",
    "print('longest train title length',train_max_title_len)\n",
    "print(len(train_text))\n",
    "print(len(train_title))\n",
    "print(train_labels.shape)\n",
    "\n",
    "\"\"\"\n",
    "longet train comment length 244\n",
    "longest train title length 13\n",
    "1528\n",
    "1528\n",
    "(1528,)\n",
    "\"\"\"\n",
    "\n",
    "print('longest test comment length', test_max_len)\n",
    "print('longest test title length', test_max_title_len)\n",
    "print(len(test_text))\n",
    "print(len(test_title))\n",
    "print(test_labels.shape)\n",
    "\n",
    "\"\"\"\n",
    "longest test comment length 126\n",
    "longest test title length 19\n",
    "102\n",
    "102\n",
    "(102,)\n",
    "\"\"\"\n",
    "\n",
    "print(train_data_array.shape, train_title_array.shape)\n",
    "print(test_data_array.shape, test_title_array.shape)\n",
    "\"\"\"\n",
    "(244, 1528, 300) (13, 1528, 300)\n",
    "(126, 102, 300) (19, 102, 300)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "data_array.shape = (244, 1528, 300)\n",
    "data_labels.shape = (1528,)\n",
    "data is an (L,N,D) array\n",
    "L = max_length of sequence\n",
    "N = batch_size\n",
    "D = embed_dim\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module): # single direction lstm, no attention\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300):\n",
    "    super(BaseModel, self).__init__()\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = True, dropout = 0.2, bidirectional = False)\n",
    "    \n",
    "    # two linear layers for context (final hidden state) => binary classification\n",
    "    self.linear1 = nn.Linear(hidden_size, 150) \n",
    "    self.linear2 = nn.Linear(150, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, data):\n",
    "    \"\"\"\n",
    "    data is an (N, L, D) = (batch_size, max_length, embed_dim) array\n",
    "    returns an (N,1) array of binary probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm(data)\n",
    "    # hidden_states = (batch_size, max_length, hidden_size) array\n",
    "    \n",
    "    sentences = torch.sum(hidden_states, axis = 1 ) # => (batch_size,hidden_size)\n",
    "\n",
    "    return self.sigmoid(torch.squeeze(self.linear2(self.relu(self.linear1(sentences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_LSTM_Model(nn.Module): # generalized lstm class\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300, bidi = True, attention = True):\n",
    "    super(Full_LSTM_Model, self).__init__()\n",
    "    if attention: assert bidi # attention only if the LSTM is bidirectional\n",
    "    \n",
    "    self.hidden_size = hidden_size \n",
    "    self.attention = attention\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = True, dropout = 0.2, bidirectional = bidi)\n",
    "\n",
    "    # two linear layers for output of lstm: (final hidden state) => binary classification\n",
    "    self.linear1 = nn.Linear(self.hidden_size*2 if bidi else self.hidden_size, 150) \n",
    "    self.linear2 = nn.Linear(150, 1)\n",
    "\n",
    "    if self.attention: #assuming bidi\n",
    "        self.attention1 = nn.Linear(2*hidden_size, 50) # map hidden state vector to value\n",
    "        self.attention2 = nn.Linear(50, 1)\n",
    "        self.sm = nn.Softmax(dim = 1)\n",
    "    \n",
    "    self.relu = nn.ReLU()\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, data):\n",
    "    \"\"\"\n",
    "    data is an (N, L, D) = (batch_size, max_length, embed_dim) array\n",
    "    returns an (N,1) array of binary probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm(data)\n",
    "    # hidden_states = (batch_size, max_length, hidden_size) array\n",
    "    \n",
    "    \"\"\"\n",
    "    in this case, attention is a choice of coefficients which we use to weight hidden states \n",
    "    when summing them instead of adding them up with equal weighting\n",
    "    \n",
    "    TODO: add masks so that we aren't operating on all the hidden states since the padded ones don't matter!\n",
    "    TODO: average sentences in non attention case instead of summing them\n",
    "    \"\"\"\n",
    "    if self.attention:\n",
    "        weights = self.attention1(hidden_states) #(batch_size,max_length,50)\n",
    "        weights = self.relu(weights) #(batch_size,max_length,50)\n",
    "        weights = self.attention2(weights) #(batch_size,max_length,1)\n",
    "        alphas = self.sm(weights.squeeze()) #sm((batch_size,max_length)) => (batch_size,max_length)\n",
    "        \n",
    "        sentences = torch.sum(hidden_states * alphas[:,:,None], axis = 1) # (batch_size,hidden_size)\n",
    "    \n",
    "    else:\n",
    "        sentences = torch.sum(hidden_states, axis = 1)\n",
    "    \n",
    "    output = self.linear2(self.relu(self.linear1(sentences))) # => (batch_size,1)\n",
    "    output = torch.squeeze(output) # => (batch_size)\n",
    "    return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module): # bidi with attention\n",
    "  def __init__(self, hidden_size = 100, embed_dim = 300):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    #self.embedding = embed\n",
    "    \n",
    "    self.linear1 = nn.Linear(2*hidden_size, hidden_size) # map context vector to value\n",
    "    self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    self.attention1 = nn.Linear(2*hidden_size, 50) # map hidden state vector to value\n",
    "    self.attention2 = nn.Linear(50, 1)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "    self.sm = nn.Softmax(dim = 0)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_size, num_layers = 1, batch_first = False, dropout = 0.2, bidirectional = True)\n",
    "\n",
    "  def forward(self, data):\n",
    "    \"\"\"\n",
    "    data is an (L,N,D) array\n",
    "    L = max_length of sequence\n",
    "    N = batch_size\n",
    "    D = embed_dim\n",
    "    returns an (N,1) array of probabilities that each comment is hateful\n",
    "    \"\"\"\n",
    "    hidden_states, (_, _) = self.lstm(data) # (L,N,2H) array\n",
    "    weights = self.attention2(self.relu(self.attention1(hidden_states))) # (L,N,1) array\n",
    "    \n",
    "    alpha = self.sm(weights.reshape(weights.shape[:-1])) # (LxN)\n",
    "\n",
    "    hidden_states = torch.moveaxis(hidden_states, -1, 0) # (2H,N,L)\n",
    "\n",
    "\n",
    "    sentences = torch.sum(hidden_states * alpha, axis = 1)\n",
    "\n",
    "    sentences = torch.moveaxis(sentences, 0, -1)\n",
    "\n",
    "    return self.sigmoid(torch.squeeze(self.linear2(self.relu(self.linear1(sentences)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
